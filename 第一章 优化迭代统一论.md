# 第一章 优化迭代统一论

## 1.1 线性回归建模

$\{(x^{(i)},y^{(i)})\}$ 为一个*训练样本*，$\{(x^{(i)},y^{(i)}); i=1,...,N\}$ 为*训练样本集*

对于多自变量：

$\{(x_1^{(i)},x_2^{(i)},y^{(i)})\} ->(X^{(i)},y^{(i)})$ ， $X^{(i)}=\begin{bmatrix} x_1^{(i)} \\x_2^{(i)} \end{bmatrix}\quad$ 

![](https://raw.githubusercontent.com/xiaohuzai/math-in-AI/master/picture/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%921.PNG)

试图学习： $f(x) = W^TX+b$ 使得 $f(X^{(i)})\approx y^{(i)}$ 核心在于怎么学？不是靠猜，需要数学知识。

（监督学习，由已知的样本学习，来预测未知。）

（$W^TX$ 为两向量之间的内积）

## 1.2 无约束优化梯度分析法

**无约束优化问题**

自变量为标量，输出是标量：$\mathbb{R}->\mathbb{R}$
$$
minf(x)     \ \ \ \ \ \ \ x\in\mathbb{R}
$$
绝大多数情况自变量是向量，输出是标量：$\mathbb{R}^n->\mathbb{R}$
$$
minf(X)     \ \ \ \ \ \ \ X\in\mathbb{R}^n
$$
需要以一种代数的方法分析函数，来分析极值点。

**优化问题可能得极值点情况**

![](https://raw.githubusercontent.com/xiaohuzai/math-in-AI/master/picture/%E6%97%A0%E7%BA%A6%E6%9D%9F%E4%BC%98%E5%8C%96%E6%A2%AF%E5%BA%A6%E5%88%86%E6%9E%901.PNG)

全局最大值、全局最小值、局部最大值、局部最小值、鞍点。

如何找函数的极值？

**梯度和Hessian矩阵**

![](C:\math-in-AI\picture\无约束优化梯度分析2.PNG)

一阶导数对应梯度，二阶导数对应海森矩阵。

**二次型**

![](C:\math-in-AI\picture\无约束优化梯度分析3.PNG)

对二次型的[解释](http://dec3.jlu.edu.cn/webcourse/t000022/teach/chapter5/5_4.htm)：